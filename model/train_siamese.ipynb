{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee080a26-8f36-4678-a658-828fa73f6412",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "If running in ITDC, Jupyter might complain about not finding CUDA drivers - we disregard this because we're not using a CUDA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cac80b1-efb7-47b3-b8a3-f637ed3235ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 02:40:01.430741: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-29 02:40:01.626681: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 02:40:01.703956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 02:40:01.704004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 02:40:01.705634: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 02:40:01.715977: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 02:40:01.716653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 02:40:03.139504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-29 02:40:05.872853: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.\n",
      "2024-09-29 02:40:05.873119: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /xla/service/gpu/compiled_programs_count. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.\n",
      "2024-09-29 02:40:05.874611: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_executions. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.\n",
      "2024-09-29 02:40:05.874629: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_execution_time_usecs. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.\n",
      "2024-09-29 02:40:07.563190: I itex/core/wrapper/itex_gpu_wrapper.cc:38] Intel Extension for Tensorflow* GPU backend is loaded.\n",
      "2024-09-29 02:40:07.566670: I external/local_xla/xla/pjrt/pjrt_api.cc:67] PJRT_Api is set for device type xpu\n",
      "2024-09-29 02:40:07.566688: I external/local_xla/xla/pjrt/pjrt_api.cc:72] PJRT plugin for XPU has PJRT API version 0.33. The framework PJRT API version is 0.34.\n",
      "2024-09-29 02:40:07.593776: I external/intel_xla/xla/stream_executor/sycl/sycl_gpu_runtime.cc:134] Selected platform: Intel(R) Level-Zero\n",
      "2024-09-29 02:40:07.593808: I external/intel_xla/xla/stream_executor/sycl/sycl_gpu_runtime.cc:159] number of sub-devices is zero, expose root device.\n",
      "2024-09-29 02:40:07.595860: I external/xla/xla/service/service.cc:168] XLA service 0x55bf5f5d6570 initialized for platform SYCL (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-29 02:40:07.595887: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): Intel(R) Data Center GPU Max 1100, <undefined>\n",
      "2024-09-29 02:40:07.596062: I itex/core/devices/gpu/itex_gpu_runtime.cc:130] Selected platform: Intel(R) Level-Zero\n",
      "2024-09-29 02:40:07.596075: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.\n",
      "2024-09-29 02:40:07.596768: I external/intel_xla/xla/pjrt/se_xpu_pjrt_client.cc:97] Using BFC allocator.\n",
      "2024-09-29 02:40:07.596790: I external/xla/xla/pjrt/gpu/gpu_helpers.cc:106] XLA backend allocating 46385646796 bytes on device 0 for BFCAllocator.\n",
      "2024-09-29 02:40:07.609305: I external/local_xla/xla/pjrt/pjrt_c_api_client.cc:119] PjRtCApiClient created.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658bbd3-aec0-4a7a-abfb-771964ea5fb1",
   "metadata": {},
   "source": [
    "# Base model\n",
    "To construct the Siamese neural network, we're using two [VGG16 models](https://arxiv.org/abs/1409.1556) as feature extractors. All input images are 400x400 pixels with 3 color channels. To minimize computational cost, we freeze the pretrained CNN layers and train only the final comparison layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8558e5-ebd4-4569-9f4c-9bee73518a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 02:40:13.629272: I tensorflow/core/common_runtime/next_pluggable_device/next_pluggable_device_factory.cc:118] Created 1 TensorFlow NextPluggableDevices. Physical device type: XPU\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(400, 400, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a33b79-b536-44b8-80d3-3907a27672e3",
   "metadata": {},
   "source": [
    "# Siamese network construction\n",
    "\n",
    "We extract features for both images, flatten them, and compute their difference. This is fed into the fully-connected comparison layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f32f20-4450-47b2-9510-4d9ddc7a3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_comparison_model():\n",
    "    input_1 = layers.Input(shape=(400, 400, 3))\n",
    "    input_2 = layers.Input(shape=(400, 400, 3))\n",
    "\n",
    "    features_1 = base_model(input_1)\n",
    "    features_2 = base_model(input_2)\n",
    "\n",
    "    flattened_1 = layers.Flatten()(features_1)\n",
    "    flattened_2 = layers.Flatten()(features_2)\n",
    "\n",
    "    subtract = layers.Subtract()([flattened_1, flattened_2])\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(subtract)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c374f27f-93a8-4c67-a0e0-f524fd370289",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_model = build_comparison_model()\n",
    "comparison_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a54260-d9d4-41b4-b479-e85a58471d0c",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "The model trains on a dataset of 2000 images - one correct and one marginally incorrect (labeled) image pair for each of 500 CSS sample rulesets. See `generate_data.py` for generation of training data. Note that each reference `i` only has two *distinct* associated images:\n",
    "\n",
    "`(ref_i_right.png, ref_i_wrong.png), (ref_i_right.png, ref_i_right.png)`\n",
    "\n",
    "So, for space efficiency, we only store `ref_i_right.png` and `ref_i_wrong.png` and construct the lists of image paths manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb32662-3caf-4c7a-8987-a593aa02feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (400, 400))\n",
    "    return image / 255.0\n",
    "\n",
    "def load_pairs(image1_path, image2_path, label):\n",
    "    image1 = load_image(image1_path)\n",
    "    image2 = load_image(image2_path)\n",
    "    return (image1, image2), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972636bc-3f56-4dd4-b9ea-92579882859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_1 = [f\"dataset/ref_{i}_right.png\" for i in range(1, 501) for _ in range(2)]\n",
    "image_paths_2 = [f\"dataset/ref_{i}_{either}.png\" for i in range(1, 501) for either in [\"right\", \"wrong\"]]\n",
    "labels = [j for _ in range(1, 501) for j in range(1, -1, -1)]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((image_paths_1, image_paths_2, labels))\n",
    "train_dataset = train_dataset.map(load_pairs).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bb824-c2aa-4ecd-98bd-3855e43e75bd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb30e27-fac7-4067-a275-6fbdd3263a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 16s 422ms/step - loss: 0.4077 - accuracy: 0.8910\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.3329 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.3213 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.2979 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.2412 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.1508 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.0693 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.0086 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/envs/tensorflow-gpu/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "comparison_model.fit(train_dataset, epochs=10)\n",
    "comparison_model.save('siamese_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1433f4-7f91-47db-8c09-f09858a8340f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9df6da7c-fb5b-4bf4-85c6-9ebb1b31e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "Prediction: [[0.9223337]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('siamese.h5')\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(400, 400))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "image1_path = \"test_right_3.png\"\n",
    "image2_path = \"test_wrong_3.png\"\n",
    "\n",
    "image1 = preprocess_image(image1_path)\n",
    "image2 = preprocess_image(image2_path)\n",
    "\n",
    "input_pair = [image1, image2]\n",
    "\n",
    "prediction = model.predict(input_pair)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd3732-9b3a-4bc5-9212-cc44f16affe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
